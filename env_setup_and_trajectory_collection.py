# -*- coding: utf-8 -*-
"""Env Setup and Trajectory Collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T10Lqz0bPgMPmGY9N2ohxxudKyAacGRn
"""

import torch
from torch.distributions import MultivariateNormal

import random

class gameEnvironment:
  def __init__(self):
    self.state=(0,0)
    self.reward=0
    self.boundaries=10
    self.prize=(random.randint(-10,10),random.randint(-10,10))
  def takeAction(self,xCoord,yCoord):
    self.state=(xCoord,yCoord)
    if self.state[0]==self.prize[0] and self.state[1]==self.prize[1]:
      self.reward=100
      self.prize=(random.randint(-10,10),random.randint(-10,10))
    else:
      self.reward=-1*((xCoord-self.prize[0])**2+(yCoord-self.prize[1])**2)
    return self.reward,self.state
  def getState(self):
    return self.state,self.reward

def collectTrajectory(steps):
  user=gameEnvironment()
  actor=Actor(3,2)#3 for current state (2 numbers) and the reward(treating observation=reward) and concatenating state+observation
  #returns 2d for the action to take
  rewards=[]
  #observations=[]
  log_probs=[]
  states=[]
  for _ in range(steps):
    state,reward=user.getState()
    actorInput=torch.tensor([state[0],state[1],reward])
    newState=actor(actorInput)
    covMatrix=torch.diag(torch.full(size=(2,),fill_value=0.5))
    distr=MultivariateNormal(torch.tensor([newState[0],newState[1]]),covMatrix)
    usedState=distr.sample()
    log_prob = distr.log_prob(usedState)
    useReward,useState=gameEnvironment.takeAction(usedState[0],usedState[1])
    rewards.append(useReward)
    log_probs.append(log_prob)
    states.append(useState)