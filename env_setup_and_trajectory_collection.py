# -*- coding: utf-8 -*-
"""Env Setup and Trajectory Collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T10Lqz0bPgMPmGY9N2ohxxudKyAacGRn
"""

import torch
from torch.distributions import MultivariateNormal

import random

"""
class gameEnvironment:
  def __init__(self):
    self.state=(0,0)
    self.reward=0
    self.boundaries=10
    self.prize=(random.randint(-10,10),random.randint(-10,10))
  def takeAction(self,xCoord,yCoord):
    self.state=(xCoord,yCoord)
    if self.state[0]==self.prize[0] and self.state[1]==self.prize[1]:
      self.reward=100
      self.prize=(random.randint(-10,10),random.randint(-10,10))
    else:
      self.reward=-1*((xCoord-self.prize[0])**2+(yCoord-self.prize[1])**2)
    return self.reward,self.state
  def getState(self):
    return self.state,self.reward
  """

def collectTrajectory(actor,env,gamma):
  gamma=0.85
  #actor=Actor(3,2)#3 for current state (2 numbers) and the reward(treating observation=reward) and concatenating state+observation
  #returns 2d for the action to take
  rewards=[]
  rewards_to_go=[]
  observations=[]
  log_probs=[]
  done=True
  actions=[]
  obs=env.reset()
  while not done:
    observations.append(obs)
    mean_action=actor(torch.tensor(obs))
    obs,reward,done,_=env.step(action)
    rewards.append(reward)
    observations.append(obs)
    if len(rewards_to_go)==0:
      rewards_to_go.append(reward)
    else:
      rewards_to_go.append(rewards_to_go[-1]*gamma+reward)
    covMatrix=torch.diag(torch.full(size=(2,),fill_value=0.5))
    distr=MultivariateNormal(mean_action,covMatrix)
    action=distr.sample()
    actions.append(action)
    log_prob=distr.log_prob(action)
    log_probs.append(log_prob)
  return rewards,rewards_to_go,log_probs,actions,observations